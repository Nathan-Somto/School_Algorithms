{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tutorial Section on SKLEARN\n",
    "\n",
    "\n",
    "Scikit-learn, also known as sklearn, is an open-source, robust Python machine learning library. It was created to help simplify the process of implementing machine learning and statistical models in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Sklearn\n",
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would used the wine datasets for this tutorial, the dataset's task involves classifying wines into one of  three cultivars. The three cultivars (classes) represented in the sklearn wine dataset correspond to different types or varieties of wine grapes. These cultivars are often associated with specific wine-producing regions and have distinct characteristics that influence the flavors, aromas, and overall profiles of the wines produced from them. (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     14.23\n",
       "1     13.20\n",
       "2     13.16\n",
       "3     14.37\n",
       "4     13.24\n",
       "5     14.20\n",
       "6     14.39\n",
       "7     14.06\n",
       "8     14.83\n",
       "9     13.86\n",
       "10    14.10\n",
       "11    14.12\n",
       "12    13.75\n",
       "13    14.75\n",
       "14    14.38\n",
       "15    13.63\n",
       "16    14.30\n",
       "17    13.83\n",
       "18    14.19\n",
       "19    13.64\n",
       "20    14.06\n",
       "21    12.93\n",
       "22    13.71\n",
       "23    12.85\n",
       "24    13.50\n",
       "25    13.05\n",
       "26    13.39\n",
       "27    13.30\n",
       "28    13.87\n",
       "29    14.02\n",
       "30    13.73\n",
       "31    13.58\n",
       "32    13.68\n",
       "33    13.76\n",
       "34    13.51\n",
       "35    13.48\n",
       "36    13.28\n",
       "37    13.05\n",
       "38    13.07\n",
       "39    14.22\n",
       "40    13.56\n",
       "41    13.41\n",
       "42    13.88\n",
       "43    13.24\n",
       "44    13.05\n",
       "45    14.21\n",
       "46    14.38\n",
       "47    13.90\n",
       "48    14.10\n",
       "49    13.94\n",
       "50    13.05\n",
       "51    13.83\n",
       "52    13.82\n",
       "53    13.77\n",
       "54    13.74\n",
       "55    13.56\n",
       "56    14.22\n",
       "57    13.29\n",
       "58    13.72\n",
       "Name: alcohol, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine_data = load_wine()\n",
    "\n",
    "# Convert data to pandas dataframe\n",
    "wine_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "\n",
    "# Add the target label\n",
    "wine_df[\"WineType\"] = wine_data.target\n",
    "\n",
    "# Take a preview\n",
    "wine_df.head(5)\n",
    "# delete me below\n",
    "table = pd.crosstab(wine_df['alcohol'], wine_df['WineType'])\n",
    "table.head(5)\n",
    "wine_df.loc[wine_df['WineType'] == 0, 'alcohol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wine_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# cross tab \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcrosstab(\u001b[43mwine_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malcohol\u001b[39m\u001b[38;5;124m'\u001b[39m], wine_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWineType\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m table\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# save in a csv\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# table.to_csv('wine.csv')\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wine_df' is not defined"
     ]
    }
   ],
   "source": [
    "# cross tab \n",
    "\n",
    "table = pd.crosstab(wine_df['alcohol'], wine_df['WineType'])\n",
    "table.head(5)\n",
    "# save in a csv\n",
    "# table.to_csv('wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=16.71133933365354, pvalue=5.926412330452344e-34, df=127.84705824080136)\n"
     ]
    }
   ],
   "source": [
    "# perform ttest on alcohol and WineType\n",
    "from scipy.stats import ttest_ind\n",
    "class_0_alcohol = wine_df.loc[wine_df['WineType'] == 0, 'alcohol']\n",
    "class_1_alcohol = wine_df.loc[wine_df['WineType'] == 1, 'alcohol']\n",
    "# pick some rows from the column\n",
    "\"\"\" ttest_1 = ttest_ind(wine_df['alcohol'], wine_df['WineType'], alternative='two-sided', equal_var=False)\n",
    "ttest_1 \"\"\"\n",
    "ttest_2 = ttest_ind(class_0_alcohol, class_1_alcohol, alternative='two-sided', equal_var=False)\n",
    "print(ttest_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    float64\n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    float64\n",
      " 13  WineType                      178 non-null    int32  \n",
      "dtypes: float64(13), int32(1)\n",
      "memory usage: 18.9 KB\n"
     ]
    }
   ],
   "source": [
    "#Data Exploration\n",
    "wine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>WineType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>0.938202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>0.775035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "         WineType  \n",
       "count  178.000000  \n",
       "mean     0.938202  \n",
       "std      0.775035  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      1.000000  \n",
       "75%      2.000000  \n",
       "max      2.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Exploration\n",
    "wine_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "\n",
    "Data processing is a vital step in the machine learning workflow because data from the real world is messy. It may contain: \n",
    "\n",
    "Missing values,\n",
    "Redundant values\n",
    "Outliers\n",
    "Errors\n",
    "Noise\n",
    "\n",
    "You must deal with all of this before feeding the data to a machine learning model; otherwise, the model will incorporate these mistakes into its approximation function – it will learn to make mistakes on new instances. This is what formed the famous machine learning saying, “Garbage in, garbage out.” \n",
    "\n",
    "Another reason is that machine learning models typically require numeric data.  \n",
    "\n",
    "Other than our data being on different scales, there’s not much else wrong with our data at first glance. To combat this problem, let’s standardize the features using sklearn’s StandardScaler class; this will ensure the mean of each feature is approximately equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cheta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.51861254 -0.5622498   0.23205254 -1.16959318  1.91390522  0.80899739\n",
      "  1.03481896 -0.65956311  1.22488398  0.25171685  0.36217728  1.84791957\n",
      "  1.01300893]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data into features and label \n",
    "X = wine_df[wine_data.feature_names].copy()\n",
    "y = wine_df[\"WineType\"].copy() \n",
    "\n",
    "# Instantiate scaler and fit on features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "# Transform features\n",
    "X_scaled = scaler.transform(X.values)\n",
    "\n",
    "# View first instance\n",
    "print(X_scaled[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Trainning --- Spliting the dataset\n",
    "\n",
    "Before a machine learning model can make predictions, it must be trained on a set of data to learn an approximation function. \n",
    "\n",
    "There are several ways to split data into train and test sets, but scikit-learn has a built-in function to do this on our behalf called train_test_split(). \n",
    "\n",
    "We’ll use this function to split our data such that 70% is used to train the model and 30% is used to evaluate the model's ability to generalize to unseen instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70% \n",
      "Test size: 30%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled,\n",
    "                                                                  y,\n",
    "                                                             train_size=.7,\n",
    "                                                           random_state=0)\n",
    "\n",
    "# Check the splits are correct\n",
    "print(f\"Train size: {round(len(X_train_scaled) / len(X) * 100)}% \\n\\\n",
    "Test size: {round(len(X_test_scaled) / len(X) * 100)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model\n",
    "Thanks to sklearn, building a machine learning model is extremely simple. \n",
    "\n",
    "We are going to build three models to predict the class of wine: \n",
    "\n",
    "Logistic regression (https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "Support vector machine (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "Decision tree classifier(https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 1 0 1 0 0 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n",
      " 1 1 2 0 0 1 1 1 0 2 1 2 0 2 2 0 2]\n",
      "[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 1 0 1 0 0 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n",
      " 1 1 2 0 0 1 1 1 0 2 1 2 0 2 2 0 2]\n",
      "[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 1 0 1 0 0 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n",
      " 1 1 2 0 0 1 1 1 0 2 1 2 0 2 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instnatiating the models \n",
    "logistic_regression = LogisticRegression()\n",
    "svm = SVC()\n",
    "tree = DecisionTreeClassifier(random_state=0) #Why do use think we are using random state and why arent we using it for the others, Some Machine learning \n",
    "                                              #algorithms are prone to randomization and would not produce the same result if random state is not decleared\n",
    "\n",
    "# Training the models \n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions with each model\n",
    "log_reg_preds = logistic_regression.predict(X_test_scaled)\n",
    "svm_preds = svm.predict(X_test_scaled)\n",
    "tree_preds = tree.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation\n",
    "\n",
    "Model evaluation is done to test how well the model generalizes to unseen instances. Scikit-learn provides an array of classification and regression metrics to evaluate a trained model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        22\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        54\n",
      "   macro avg       1.00      1.00      1.00        54\n",
      "weighted avg       1.00      1.00      1.00        54\n",
      "\n",
      "Support Vector Machine Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        22\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        54\n",
      "   macro avg       1.00      1.00      1.00        54\n",
      "weighted avg       1.00      1.00      1.00        54\n",
      "\n",
      "Decision Tree Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        19\n",
      "           1       0.91      0.95      0.93        22\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.95      0.95      0.95        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Store model predictions in a dictionary\n",
    "# this makes it's easier to iterate through each model\n",
    "# and print the results. \n",
    "model_preds = {\n",
    "    \"Logistic Regression\": log_reg_preds,\n",
    "    \"Support Vector Machine\": svm_preds,\n",
    "    \"Decision Tree\": tree_preds\n",
    "}\n",
    "\n",
    "for model, preds in model_preds.items():\n",
    "    print(f\"{model} Results:\\n{classification_report(y_test, preds)}\", sep=\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks\n",
    "\n",
    "1. Change the random state of the Decision tree classifier (for example set it to 42), what was the effect of this change\n",
    "2. Conduct an experiment using the 3 Machine learning algorithms onthe Sklearn breast cancer dataset (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer)\n",
    "3. what is the performance of the random forest algorithm on the breast cancer dataset (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a type of unsupervised learning technique used to group data points or objects based on their similarity. The goal of clustering is to identify inherent patterns or structures in the data without prior knowledge of true labels. Clustering algorithms partition the data into groups or clusters such that data points within the same cluster are more similar to each other than to those in other clusters.\n",
    "\n",
    "In this tutorial we would use two clustering algorithms: K-means and Agglomerative Clustering \n",
    "\n",
    "We would use metrics such as Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Score and Adjusted Rand Index (ARI)\n",
    "\n",
    "K-means Clustering:\n",
    " K-means is a popular centroid-based clustering algorithm. It partitions the data into K clusters by iteratively assigning data points to the nearest cluster centroid and updating the centroids based on the mean of data points assigned to each cluster.\n",
    "\n",
    "\n",
    "Agglomerative Clustering:\n",
    "Agglomerative clustering is a hierarchical clustering method that starts with each data point as a separate cluster and merges clusters iteratively based on a linkage criterion (e.g., distance between clusters)\n",
    "\n",
    "\n",
    "Performance Metrics:\n",
    "Silhouette Score: Measures how similar a data point is to its own cluster compared to other clusters. Higher score indicates dense and well-separated clusters.\n",
    "\n",
    "Davies-Bouldin Index: Computes the average similarity between each cluster and its most similar cluster, where lower values indicate better clustering.\n",
    "\n",
    "Calinski-Harabasz Score: Ratio of within-cluster dispersion to between-cluster dispersion, higher values indicate better-defined clusters.\n",
    "\n",
    "Adjusted Rand Index (ARI): Compares the similarity of true cluster assignments with the clustering results, providing a measure of cluster accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "data = load_wine()\n",
    "X = data.data  # Features\n",
    "y = data.target  # True labels (for adjusted Rand index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clustering algorithms\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the clustering algorithms to the scaled data\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "agg_labels = agg_clustering.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-means clustering performance using multiple metrics\n",
    "metrics_kmeans = {\n",
    "    'Silhouette Score': silhouette_score(X_scaled, kmeans_labels),\n",
    "    'Davies-Bouldin Index': davies_bouldin_score(X_scaled, kmeans_labels),\n",
    "    'Calinski-Harabasz Score': calinski_harabasz_score(X_scaled, kmeans_labels),\n",
    "    'Adjusted Rand Index': adjusted_rand_score(y, kmeans_labels)  # Using true labels for ARI\n",
    "}\n",
    "\n",
    "print(\"K-means Clustering Performance:\")\n",
    "for metric_name, metric_value in metrics_kmeans.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Agglomerative Clustering performance using multiple metrics\n",
    "metrics_agg = {\n",
    "    'Silhouette Score': silhouette_score(X_scaled, agg_labels),\n",
    "    'Davies-Bouldin Index': davies_bouldin_score(X_scaled, agg_labels),\n",
    "    'Calinski-Harabasz Score': calinski_harabasz_score(X_scaled, agg_labels),\n",
    "    'Adjusted Rand Index': adjusted_rand_score(y, agg_labels)  # Using true labels for ARI\n",
    "}\n",
    "\n",
    "print(\"\\nAgglomerative Clustering Performance:\")\n",
    "for metric_name, metric_value in metrics_agg.items():\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Knew that there were 3 cultivars in the wine dataset, what if we didnt there are several methods that can be used to determine the optimal number of clusters if the number of clusters are not known. Some of them include Elbow Method, Silhouette Analysis, Gap Statistic method, and Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow Method:\n",
    "The elbow method involves plotting the within-cluster sum of squares (inertia) against the number of clusters (K) and identifying the \"elbow\" point where the rate of decrease in inertia sharply decreases. This point represents a good estimate for the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m inertias \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m     kmeans\u001b[38;5;241m.\u001b[39mfit(X_scaled)\n\u001b[0;32m      5\u001b[0m     inertias\u001b[38;5;241m.\u001b[39mappend(kmeans\u001b[38;5;241m.\u001b[39minertia_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "inertias = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertias, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Analysis:\n",
    "Silhouette analysis measures how well each data point fits into its assigned cluster and can be used to determine the optimal number of clusters. The highest average silhouette score across different numbers of clusters indicates the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis for Optimal K')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gap Statistic:\n",
    "The gap statistic compares the within-cluster dispersion of the data to a reference null distribution and helps identify the optimal number of clusters by maximizing the gap statistic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def compute_gap_statistic(data, k_range, n_ref_samples=10, random_seed=None):\n",
    "    \"\"\"\n",
    "    Compute the Gap Statistic for estimating the optimal number of clusters.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): Input data matrix (n_samples, n_features).\n",
    "        k_range (list): List of integers specifying the range of k values (number of clusters) to evaluate.\n",
    "        n_ref_samples (int): Number of reference samples to generate for calculating the reference distribution.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Tuple containing the calculated gap statistics and standard deviations for each k value.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Initialize arrays to store gap statistics and standard deviations\n",
    "    gap_stats = []\n",
    "    gap_stds = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Fit KMeans clustering to the data\n",
    "        kmeans_model = KMeans(n_clusters=k, random_state=random_seed)\n",
    "        kmeans_model.fit(data)\n",
    "        \n",
    "        # Calculate the within-cluster dispersion (log of sum of square distances)\n",
    "        Wk = np.log(kmeans_model.inertia_)\n",
    "        \n",
    "        # Generate reference datasets and calculate their within-cluster dispersions\n",
    "        ref_Wks = []\n",
    "        for _ in range(n_ref_samples):\n",
    "            # Generate reference dataset with the same shape and distribution as the original data\n",
    "            ref_data = np.random.rand(*data.shape)\n",
    "            \n",
    "            # Fit KMeans to reference dataset\n",
    "            ref_kmeans_model = KMeans(n_clusters=k, random_state=random_seed)\n",
    "            ref_kmeans_model.fit(ref_data)\n",
    "            \n",
    "            # Calculate within-cluster dispersion of reference dataset\n",
    "            ref_Wk = np.log(ref_kmeans_model.inertia_)\n",
    "            ref_Wks.append(ref_Wk)\n",
    "        \n",
    "        # Calculate Gap Statistic and its standard deviation\n",
    "        gap_stat = np.mean(ref_Wks) - Wk\n",
    "        gap_std = np.std(ref_Wks) * np.sqrt(1 + 1/n_ref_samples)\n",
    "        \n",
    "        gap_stats.append(gap_stat)\n",
    "        gap_stds.append(gap_std)\n",
    "    \n",
    "    return np.array(gap_stats), np.array(gap_stds)\n",
    "\n",
    "\n",
    "# Define the range of k values (number of clusters) to evaluate\n",
    "k_range = range(1, 11)\n",
    "\n",
    "# Compute Gap Statistic for the range of k values\n",
    "gap_stats, gap_stds = compute_gap_statistic(X_scaled, k_range, n_ref_samples=10, random_seed=42)\n",
    "\n",
    "# Plotting the Gap Statistic curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, gap_stats, marker='o', color='b', label='Gap Statistic')\n",
    "plt.errorbar(k_range, gap_stats, yerr=gap_stds, fmt='-o', color='b', alpha=0.5, label='Gap Statistic with Std Dev')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Gap Statistic')\n",
    "plt.title('Gap Statistic for Optimal k')\n",
    "plt.xticks(k_range)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering (Dendrogram):\n",
    "Hierarchical clustering can provide insights into the underlying structure of the data by visualizing a dendrogram, which represents the hierarchical merging of clusters. The height at which branches are merged can help determine the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(Z)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: repeat the clustering tasks for the Sklearn BRCA dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
